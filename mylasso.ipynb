{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glmnet import LogitNet\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import csv\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer system spec\n",
    "MacBook Air (13-inch, Early 2014)\n",
    "\n",
    "Processor 1.4 GHz Intel Core i5\n",
    "\n",
    "Memory 4 GB 1600 MHz DDR3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset Ames_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('/Users/glen/Desktop/Statistical learning/Project/Ames_data.csv')\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for each column\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2930, 83)\n",
      "Test number: 879\n",
      "unique PID number 2930\n"
     ]
    }
   ],
   "source": [
    "# df shape\n",
    "testnum = int(round(df.shape[0]*0.3 , 0))\n",
    "print(df.shape)\n",
    "print(\"Test number:\", testnum)\n",
    "\n",
    "#make sure PID is unique\n",
    "print(\"unique PID number\",pd.unique(df['PID']).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Because I try to use one-hot encoding later. In order to decrease the number of categories. First I delete the column if its categories exceed 15. Training dataset and test dataset would get different number of columns while doing one-hot encoding, so I need to align the X_train and X_test together to get the same number of column. Then I use imputer to replace the Nan value by the mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 2930 rows and 69 columns\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "\n",
    "#delete column 'Street', 'Longtitude', 'Latitude'...\n",
    "df = df.drop(columns = ['Street', 'Utilities', 'Land_Slope', 'Condition_2', 'Roof_Matl', 'Heating', 'Pool_QC', 'Misc_Feature', 'Low_Qual_Fin_SF', 'Three_season_porch', 'Pool_Area', 'Misc_Val', 'Longitude', 'Latitude'])\n",
    "\n",
    "# #Choose column\n",
    "# choose_column = [col for col in X_train.columns]\n",
    "\n",
    "# X_train = X_train[choose_column]\n",
    "# X_test = X_test[choose_column]\n",
    "\n",
    "print(\"dataset contains {0} rows and {1} columns\".\n",
    "      format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset after one hot contains 879 rows and 303 columns\n"
     ]
    }
   ],
   "source": [
    "#implement one hot encoding to get the result\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "print(\"dataset after one hot contains {0} rows and {1} columns\".\n",
    "      format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into 70% training and 30% testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 2051 rows and 307 columns\n",
      "Test dataset contains 879 rows and 307 columns\n"
     ]
    }
   ],
   "source": [
    "# Split data in input and output\n",
    "original_y = df['Sale_Price']\n",
    "original_X = df.drop(columns = ['Sale_Price'])\n",
    "\n",
    "#Split data in train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    original_X, original_y, test_size=0.3,random_state=12)\n",
    "\n",
    "print(\"Train dataset contains {0} rows and {1} columns\".\n",
    "      format(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"Test dataset contains {0} rows and {1} columns\".\n",
    "      format(X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "#Save the train.csv and test.csv file\n",
    "\n",
    "X_train.to_csv('train.csv', index = False)\n",
    "X_test.join(y_test).to_csv('test.csv', index = False)\n",
    "\n",
    "# pd.merge(X_test,y_test,left_on='index',right_on='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 2051 rows and 307 columns\n",
      "Test dataset contains 879 rows and 307 columns\n"
     ]
    }
   ],
   "source": [
    "#Use Imputer to deal with missing value or nan\n",
    "#train_X and test_X are numpy array\n",
    "my_imputer = Imputer(missing_values='NaN', strategy='mean')\n",
    "train_X = my_imputer.fit_transform(X_train)\n",
    "test_X = my_imputer.transform(X_test)\n",
    "print(\"Train dataset contains {0} rows and {1} columns\".\n",
    "      format(train_X.shape[0], train_X.shape[1]))\n",
    "print(\"Test dataset contains {0} rows and {1} columns\".\n",
    "      format(test_X.shape[0], test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save X_train and y_train in csv files. Then I use glmnet in R to predict the best lambda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X_train & X_test as csv file which are used to compute best lambda\n",
    "# best lambda = 763.7335\n",
    "X_train.fillna(0)\n",
    "X_train.to_csv('X_train.csv', index = False)\n",
    "y_train.to_csv('y_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Descent lasso algorithm\n",
    "Use coordinate descent to calculate the result. Unfortunately, I can't get a decent result from this model. My smallest RMSE value is 0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Lasso using Coordinate Descent\n",
    "def one_step_lasso(r, x, lam):\n",
    "    # x: nx1 matrix\n",
    "    # r: nx1 matrix\n",
    "    # Use the soft-thresholding result lasso_j = sgn(βLS_j)(|βLSj|−γ)+\n",
    "    # return beta_j\n",
    "    xx = np.sum(np.square(x))\n",
    "    xr = np.sum(x*r)\n",
    "    b = (abs(xr) -lam/2)/xx\n",
    "    b = np.sign(xr)*(b if b>0 else 0) \n",
    "    return b\n",
    "\n",
    "def mylasso(X, y, lam, n_iter, standardize = False):\n",
    "    # X: n-by-p design matrix without the intercept\n",
    "    # y: n-by-1 response vector\n",
    "    # p: p-by-1 vector\n",
    "    # lam: lambda value\n",
    "    # n.iter: number of iterations\n",
    "    # standardize: if True, center and scale X and y. \n",
    "    \n",
    "    #p is the number of features (columns)\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # n is the number of rows\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # YOUR CODE\n",
    "    # If standardize  = TRUE, center and scale X and Y \n",
    "    # record the corresponding means and sd\n",
    "    # scaler = StandardScaler()\n",
    "    # X = scaler.fit_transform(X)\n",
    "    # y = scaler.fit_transform(y)\n",
    "    mean_y = np.mean(y)\n",
    "    mean_X = np.mean(X,axis = 0)\n",
    "    sd_y = np.std(y)\n",
    "    sd_X = np.std(X,axis = 0)\n",
    "    \n",
    "    if standardize == True:\n",
    "        mean_y = np.mean(y)\n",
    "        mean_X = np.mean(X,axis = 0)\n",
    "        sd_y = np.std(y)\n",
    "        sd_X = np.std(X,axis = 0)\n",
    "        X = (X - np.mean(X,axis = 0))/np.std(X,axis = 0)\n",
    "        y = (y - np.mean(y))/np.std(y)\n",
    "\n",
    "    # Initial values for residual and coefficient vector b\n",
    "    # b: p-by-1 vector, without intercept\n",
    "        \n",
    "    b = np.zeros(p)\n",
    "    r = y\n",
    "\n",
    "    \n",
    "    for step in range(n_iter):\n",
    "        for j in range(p):\n",
    "        # YOUR CODE \n",
    "        # 1) Update the residual vector to be the one\n",
    "        # in blue on p37 of [lec_W3_VariableSelection.pdf]. \n",
    "        # r <-- current residual + X[, j] * b[j]\n",
    "        # r is n-by-1 vector\n",
    "\n",
    "            r = r + X[:,j]*b[j]\n",
    "        # 2) Apply one_step_lasso to update beta_j\n",
    "        # b[j] = one_step_lasso(r, X[, j], lam)\n",
    "            b[j] = one_step_lasso(r, X[:,j], lam)\n",
    "\n",
    "#         # 3) Update the current residual vector\n",
    "#         # r <-- r - X[, j] * b[j]\n",
    "            r = r - X[:,j]*b[j]\n",
    "    # YOUR CODE: scale back b and add intercept b0\n",
    "    # For b0, check p13 of [lec_W3_VariableSelection.pdf]. \n",
    "\n",
    "#     b = (sd_y/sd_X)*b\n",
    "    b_intercept = mean_y - np.dot(mean_X.T,b)\n",
    "    return (b_intercept,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glen/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "lam = 0.013\n",
    "X = train_X\n",
    "y = y_train\n",
    "n_iter = 100\n",
    "standardize = True\n",
    "b_intercept = mylasso(X, y, lam, n_iter)[0]\n",
    "b = mylasso(X, y, lam, n_iter)[1].reshape(307,1)\n",
    "# np.unique(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-Mean-Squared-Error (RMSE) for Coordinate decent lasso model is 0.9146\n"
     ]
    }
   ],
   "source": [
    "pre_test_y = np.dot(test_X,b) + b_intercept\n",
    "ans = round(math.sqrt(np.mean(np.square(\n",
    "    np.log(abs(pre_test_y)) - np.array(np.log(y_test))))), 4)\n",
    "print(\"Root-Mean-Squared-Error (RMSE) for Coordinate decent lasso model is {0}\".\n",
    "      format(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the result as mysubmission3.txt\n",
    "PID = np.array(X_test['PID'].values).reshape(len(X_test),1)\n",
    "Sale_Price = pre_test_y.reshape(len(pre_test_y),1)\n",
    "ans = np.concatenate((PID,Sale_Price),axis=1)\n",
    "\n",
    "# save the data with mixted datatype, %d saves PID as integer and \n",
    "# %10.5f can save the sale_price as double \n",
    "np.savetxt(\"mysubmission3\", ans, delimiter=',', \n",
    "           header=\"PID,Sale_Price\", fmt='%d %10.5f', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
